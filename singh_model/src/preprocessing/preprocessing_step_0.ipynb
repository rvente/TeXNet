{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex\n",
    "\n",
    "&copy; Copyright 2017-2018 Sumeet S Singh\n",
    "\n",
    "    This file is part of im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.width = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Read Before You Begin\n",
    "**Only follow these steps if you want to generate additional data on top of what was used to generate the resuls presented in the accompanying paper.**\n",
    "\n",
    "On the other hand, if you are content with the dataset that was used for this project and/or want to compare your model with mine, then skip this step and start wtih step1 instead i.e. move on to notebook preprocessing_step_1...\n",
    "\n",
    "This notebook will guide you how to download, process and save latex-formulas for the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Manual Steps\n",
    "\n",
    "#### Download Github repos\n",
    "Download contents of [untrix/im2latex](https://github.com/untrix/im2latex) to ~/im2latex and [untrix/im2latex-dataset](https://github.com/untrix/im2latex-dataset) to ~/im2latex-dataset.\n",
    "\n",
    "#### Download LaTeX documents from the web\n",
    "Follow instructions at [untrix/im2latex-dataset](https://github.com/untrix/im2latex-dataset) and download a bunch of tar.gz files of latex documents. The instructions provide a bunch of URLs to the KDD Cup 2003 dataset from where you can download the data.\n",
    "In the example below the files are downloaded into ~/im2latex/data/hep_downloads/hep-ph/tars - but feel free to download them wherever you want and replace the directory names in the instructions appropriately.\n",
    "\n",
    "#### Extract Math formulas from the LaTeX documents\n",
    "Follow instructions at [untrix/im2latex-dataset](https://github.com/untrix/im2latex-dataset) on running latex2formulas.py\n",
    "\n",
    "> `cd ~/im2latex/data/hep_downloads/hep-ph`  \n",
    "> `python ~/im2latex-dataset/src/latex2formulas.py tars/`\n",
    "\n",
    "This will produce a file called formulas.txt. Copy it out.\n",
    "\n",
    "> `cp formulas.txt ~/im2latex/data/dataset_tmp`\n",
    "\n",
    "#### Normalize formulas\n",
    "Here, we'll use the preprocessing code from [harvardnlp/im2markup](https://github.com/harvardnlp/im2markup). For convenience, the preprocessing code has been copied into the folder thirdparty/harvardnlp-im2markup in this repo. and some minor changes made thereafter.\n",
    "\n",
    "Note that this script internally uses node-js, so you'll need to install node-js before you run it.\n",
    "\n",
    "> `cd ~/im2latex/thirdparty/harvardnlp_im2markup`  \n",
    "> `python scripts/preprocessing/preprocess_formulas.py --mode normalize --input-file ~/im2latex/data/dataset_tmp/formulas.txt --output-file ~/im2latex/data/dataset_tmp/formulas.norm.txt`\n",
    "\n",
    "This produces a text file ~/im2latex/data/dataset_tmp/formulas.norm.txt with normalized formulas. This is to spit-out 100s of errors as it encounters sytactically incorrect formulas but that's okay because that way you weed out the bad formulas. I do not expect any other errors or issues in producing the normalized output file. If you do run into issues, look at the repo [harvardnlp/im2markup](https://github.com/harvardnlp/im2markup) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the normalized formulas and process them further. Run the following cells in a live jupyter notebook or iPython environment. **Be sure to change the file and directory names below as appropriate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = '../data/dataset_tmp'\n",
    "formula_file = os.path.join(input_dir, 'formulas.norm.txt')\n",
    "output_dir = '../data/dataset_tmp/step0'\n",
    "output_file = os.path.join(output_dir, 'formulas.norm.filtered.txt')\n",
    "dump = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readlines_to_df(path, colname):\n",
    "#   return pd.read_csv(output_file, sep='\\t', header=None, names=['formula'], index_col=False, dtype=str, skipinitialspace=True, skip_blank_lines=True)\n",
    "    rows = []\n",
    "    n = 0\n",
    "    with open(path, 'r') as f:\n",
    "        print 'opened file %s'%path\n",
    "        for line in f:\n",
    "            n += 1\n",
    "            line = line.strip()  # remove \\n\n",
    "            if len(line) > 0:\n",
    "                rows.append(line.encode('utf-8'))\n",
    "    print 'processed %d lines resulting in %d rows'%(n, len(rows))\n",
    "    return pd.DataFrame({colname:rows}, dtype=np.str_)\n",
    "\n",
    "def readlines_to_sr(path):\n",
    "    rows = []\n",
    "    n = 0\n",
    "    with open(path, 'r') as f:\n",
    "        print 'opened file %s'%path\n",
    "        for line in f:\n",
    "            n += 1\n",
    "            line = line.strip()  # remove \\n\n",
    "            if len(line) > 0:\n",
    "                rows.append(line.encode('utf-8'))\n",
    "    print 'processed %d lines resulting in %d rows'%(n, len(rows))\n",
    "    return pd.Series(rows, dtype=np.str_)\n",
    "\n",
    "def sr_to_lines(sr, path):\n",
    "#   df.to_csv(path, header=False, index=False, columns=['formula'], encoding='utf-8', quoting=csv.QUOTE_NONE, escapechar=None, sep='\\t')\n",
    "    assert sr.dtype == np.str_ or sr.dtype == np.object_\n",
    "    with open(path, 'w') as f:\n",
    "        for s in sr:\n",
    "            assert '\\n' not in s\n",
    "            f.write(s.strip())\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened file ../data/dataset_tmp/formulas.norm.txt\n",
      "processed 338921 lines resulting in 338142 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(338142,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all = readlines_to_df(formula_file, 'formula')\n",
    "# df_all.shape\n",
    "sr_all = readlines_to_sr(formula_file)\n",
    "sr_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338142,)\n",
      "(338078,)\n"
     ]
    }
   ],
   "source": [
    "# sr_formula = df_all.formula.str.strip()\n",
    "# df_stripped = df_all.drop('formula', axis='columns').assign(formula=sr_formula)\n",
    "# print df_stripped.shape\n",
    "# df_len_filtered = df_stripped[(sr_formula.str.split().str.len() > 3)]\n",
    "# print df_len_filtered.shape\n",
    "sr_stripped = sr_all.str.strip()\n",
    "print sr_stripped.shape\n",
    "sr_len_filtered = sr_stripped[(sr_stripped.str.split().str.len() > 3)]\n",
    "print sr_len_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove text-only 'formulas'\n",
    "It turns out that some of the downloaded formulas were only english sentences - i.e. they didn't have any mathematics in them. Hence we'll discard those by retaining only formulas that have at least one LaTeX command. We'll detect this by the presence of a backslash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337565,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_words(sr_, words):\n",
    "    sr = sr_\n",
    "    for word in words:\n",
    "        sr = sr[~(sr.str.contains(word))]\n",
    "    return sr\n",
    "\n",
    "sr2 = filter_words(sr_len_filtered, [r'%', r'\\label', r'\\cite', r'\\ref', r'\\pageref', r'\\footnote'])\n",
    "# df2 =df2[~df2['formula'].str.contains('%')]  # Remove all commented out lines (will remove any line with a comment)\n",
    "# df2 = df2[~df2['formula'].str.contains(r'\\label')]\n",
    "# df2 = df2[~df2['formula'].str.contains(r'\\cite')]\n",
    "# df2 =df2[df2['formula'].str.contains(r'\\\\')]\n",
    "# df2 = df2[df2['formula'].str.contains(r'=')]\n",
    "# df2 = df2[~df2['formula'].str.contains(r'\\ref')]\n",
    "# df2 = df2[~df2['formula'].str.contains(r'\\pageref')]\n",
    "# df2 = df2[~df2['formula'].str.contains(r'\\footnote')]\n",
    "sr2.shape  # 337565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample a fraction of the formulas\n",
    "Since we ended up with more formulas than we need, we'll sample a fraction for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67513,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_final = sr2.sample(frac=0.2)\n",
    "sr_final.shape  # 67513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dump:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    sr_to_lines(sr_final, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check The Saved File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened file ../data/dataset_tmp/step0/formulas.norm.filtered.txt\n",
      "processed 67513 lines resulting in 67513 rows\n"
     ]
    }
   ],
   "source": [
    "sr_read = readlines_to_sr(output_file)\n",
    "assert sr_read.shape[0] == sr_final.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
