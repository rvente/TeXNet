\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Yu2015MultiScaleCA}
\citation{Vaswani2017AttentionIA}
\citation{Oord2016PixelRN,Oord2016ConditionalIG,Salimans2017PixelCNNIT,Theis2015GenerativeIM}
\citation{Karpathy2015DeepVA,Xu2015ShowAA,Johnson2016DenseCapFC,DBLP:journals/corr/PedersoliLSV16,Vinyals2015ShowAT}
\citation{Donahue2015LongtermRC}
\citation{Oord2016WaveNetAG}
\citation{Graves2008OfflineHR,Bluche2016JointLS}
\citation{Graves2013GeneratingSW}
\citation{Cho2014LearningPR,Bahdanau2014NeuralMT,Kalchbrenner2016NeuralMT,Sutskever2014SequenceTS}
\citation{Graves2006ConnectionistTC,DBLP:journals/corr/ChanJLV15,DBLP:journals/corr/abs-1303-5778}
\citation{Graves2008SupervisedSL,Vaswani2017AttentionIA}
\citation{Cho2014LearningPR}
\citation{Sutskever2014SequenceTS}
\citation{Bahdanau2014NeuralMT,DBLP:journals/corr/LuongPM15}
\citation{Xu2015ShowAA}
\citation{Xu2015ShowAA}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\citation{Xu2015ShowAA}
\citation{Bahdanau2014NeuralMT}
\citation{Sutskever2014SequenceTS,Cho2014LearningPR}
\citation{Bahdanau2014NeuralMT}
\citation{Graves2008SupervisedSL}
\citation{Deng2017ImagetoMarkupGW}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The \textsc  {Im2Latex} problem}{2}{subsection.1.1}\protected@file@percent }
\newlabel{the_problem}{{1.1}{2}{The \textsc {Im2Latex} problem}{subsection.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A training sample: At the top is the input image $\boldsymbol  {x}$, middle the target sequence $\boldsymbol  {y}$ ($\tau = 145$) and bottom the predicted sequence $\boldsymbol  {\mathaccentV {hat}05E{y}}$ (${\tau } = 148$). Each space-separated word in $\boldsymbol  {y}$ and $\boldsymbol  {\mathaccentV {hat}05E{y}}$ $\in $ $V$ \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-sample}{{1}{2}{A training sample: At the top is the input image $\boldsymbol {x}$, middle the target sequence $\boldsymbol {y}$ ($\tau = 145$) and bottom the predicted sequence $\boldsymbol {\hat {y}}$ (${\tau } = 148$). Each space-separated word in $\boldsymbol {y}$ and $\boldsymbol {\hat {y}}$ $\in $ $V$ \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Image to markup model}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Encoder}{2}{subsection.2.1}\protected@file@percent }
\newlabel{encoder-brief}{{2.1}{2}{Encoder}{subsection.2.1}{}}
\citation{Sutskever2014SequenceTS}
\citation{Hochreiter:1997:LSM:1246443.1246450}
\citation{Pascanu2013HowTC}
\newlabel{fig-i2l-brief}{{2a}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@fig-i2l-brief}{{a}{3}{\relax }{figure.caption.3}{}}
\newlabel{fig-decoder}{{2b}{3}{\relax }{figure.caption.3}{}}
\newlabel{sub@fig-decoder}{{b}{3}{\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{eqn-a}{{2}{3}{Encoder}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decoder}{3}{subsection.2.2}\protected@file@percent }
\newlabel{decoder-brief}{{2.2}{3}{Decoder}{subsection.2.2}{}}
\newlabel{eqn-decoder-defns}{{3}{3}{Decoder}{equation.2.3}{}}
\newlabel{eqn-sequence-prob}{{4}{3}{Decoder}{equation.2.4}{}}
\citation{Graves2008SupervisedSL}
\citation{Xu2015ShowAA}
\citation{Bahdanau2014NeuralMT}
\citation{Xu2015ShowAA}
\citation{Graves2013GeneratingSW}
\citation{DBLP:journals/corr/abs-1303-5778}
\citation{Zaremba2014RecurrentNN,Pascanu2013HowTC}
\citation{Pascanu2013HowTC}
\newlabel{eqn-rnn}{{5}{4}{Decoder}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Inferencing}{4}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Visual attention and alignment model}{4}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{attention}{{2.2.2}{4}{Visual attention and alignment model}{subsubsection.2.2.2}{}}
\newlabel{eqn-alpha}{{6}{4}{Visual attention and alignment model}{equation.2.6}{}}
\newlabel{eqn-fatt}{{7}{4}{Visual attention and alignment model}{equation.2.7}{}}
\newlabel{eqn-z}{{8}{4}{Visual attention and alignment model}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}LSTM stack}{4}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{lstm-stack}{{2.2.3}{4}{LSTM stack}{subsubsection.2.2.3}{}}
\newlabel{eqn-lstm-stack}{{9}{4}{LSTM stack}{equation.2.9}{}}
\citation{Xu2015ShowAA}
\citation{Kingma2014AdamAM}
\citation{Graves2006ConnectionistTC}
\citation{Graves2006ConnectionistTC}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visual Attention}}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig-att}{{3}{5}{Visual Attention}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Deep output layer}{5}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Init model}{5}{subsubsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Init Model. FC = Fully Connected Layer.\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig-init}{{4}{5}{Init Model. FC = Fully Connected Layer.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training}{5}{subsection.2.3}\protected@file@percent }
\newlabel{training}{{2.3}{5}{Training}{subsection.2.3}{}}
\newlabel{eqn-J}{{11}{5}{Training}{equation.2.11}{}}
\citation{Deng2017ImagetoMarkupGW}
\citation{Deng2017ImagetoMarkupGW}
\citation{Deng2017ImagetoMarkupGW}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Training metrics. $\lambda _R=0.00005 \text  {\nobreakspace  {}and\nobreakspace  {}} \beta _2 = 0.9$ for all runs. The number after @ sign is the training epoch of the selected model-snapshot. $^*$ denotes that the row corresponds to Table \ref  {table-scores}.\relax }}{6}{table.caption.12}\protected@file@percent }
\newlabel{table-training1}{{1}{6}{Training metrics. $\lambda _R=0.00005 \text {~and~} \beta _2 = 0.9$ for all runs. The number after @ sign is the training epoch of the selected model-snapshot. $^*$ denotes that the row corresponds to Table \ref {table-scores}.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{6}{section.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test results. Im2latex-100k results are from \citet  {Deng2017ImagetoMarkupGW}. The last column is the percentage of successfully rendering predictions.\relax }}{6}{table.caption.15}\protected@file@percent }
\newlabel{table-scores}{{2}{6}{Test results. Im2latex-100k results are from \citet {Deng2017ImagetoMarkupGW}. The last column is the percentage of successfully rendering predictions.\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A Sample of Correct Predictions}}{7}{figure.caption.13}\protected@file@percent }
\newlabel{fig-good-preds}{{5}{7}{A Sample of Correct Predictions}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Random Sample of Mistakes}}{7}{figure.caption.14}\protected@file@percent }
\newlabel{fig-bad-preds}{{6}{7}{Random Sample of Mistakes}{figure.caption.14}{}}
\citation{Deng2017ImagetoMarkupGW,Bahdanau2014NeuralMT}
\citation{Liu2017AttentionCI}
\citation{DBLP:journals/corr/PedersoliLSV16}
\citation{Deng2017ImagetoMarkupGW}
\citation{Deng2017ImagetoMarkupGW}
\citation{Xu2015ShowAA}
\citation{Deng2017ImagetoMarkupGW}
\citation{Bluche2014ACO}
\bibstyle{apalike}
\bibdata{I2LPaper_NIPS2018}
\citation{Simonyan2014VeryDC}
\citation{Xu2015ShowAA}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model Interpretability via attention}{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Object detection via attention:}{8}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dataset}{8}{subsection.3.2}\protected@file@percent }
\newlabel{dataset}{{3.2}{8}{Dataset}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Ancillary material}{8}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Qualitative analyses and details}{8}{appendix.A}\protected@file@percent }
\newlabel{observations}{{A}{8}{Qualitative analyses and details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Encoder}{8}{subsection.A.1}\protected@file@percent }
\newlabel{encoder-commentary}{{A.1}{8}{Encoder}{subsection.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Random Sample of Predictions}}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig-rand-preds}{{7}{9}{Random Sample of Predictions}{figure.caption.19}{}}
\citation{Xu2015ShowAA}
\citation{Xu2015ShowAA}
\citation{Xu2015ShowAA}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Specification of the Encoder CNN.\relax }}{10}{table.caption.20}\protected@file@percent }
\newlabel{table-cnn}{{3}{10}{Specification of the Encoder CNN.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Attention model}{10}{subsection.A.2}\protected@file@percent }
\newlabel{att-analyses}{{A.2}{10}{Attention model}{subsection.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Visual Attention MLP}}{10}{table.caption.21}\protected@file@percent }
\newlabel{table-att}{{4}{10}{Visual Attention MLP}{table.caption.21}{}}
\citation{DBLP:journals/corr/abs-1303-5778,Zaremba2014RecurrentNN}
\citation{Xu2015ShowAA}
\citation{Xu2015ShowAA}
\citation{Xu2015ShowAA}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}LSTM stack}{11}{subsection.A.3}\protected@file@percent }
\newlabel{lstm-comments}{{A.3}{11}{LSTM stack}{subsection.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces LSTM}}{11}{figure.caption.22}\protected@file@percent }
\newlabel{fig-lstm}{{8}{11}{LSTM}{figure.caption.22}{}}
\newlabel{eqn-lstm}{{12}{11}{LSTM stack}{equation.A.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Deep output layer}{11}{subsection.A.4}\protected@file@percent }
\newlabel{output-comments}{{A.4}{11}{Deep output layer}{subsection.A.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Output Layer Configuration}}{11}{table.caption.23}\protected@file@percent }
\newlabel{table-output-layer}{{5}{11}{Output Layer Configuration}{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Init model}{11}{subsection.A.5}\protected@file@percent }
\newlabel{init-model-comments}{{A.5}{11}{Init model}{subsection.A.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Init Model layers.\relax }}{11}{table.caption.24}\protected@file@percent }
\newlabel{table-init-model}{{6}{11}{Init Model layers.\relax }{table.caption.24}{}}
\citation{Deng2017ImagetoMarkupGW}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Impact of the Init Model on overall performance. Since it comprises 10-12\% of the total params, it may as well be omitted in exchange for a small performance hit.\relax }}{12}{table.caption.25}\protected@file@percent }
\newlabel{table-init-efficacy}{{7}{12}{Impact of the Init Model on overall performance. Since it comprises 10-12\% of the total params, it may as well be omitted in exchange for a small performance hit.\relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Training and dataset}{12}{subsection.A.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6.1}Alpha penalty}{12}{subsubsection.A.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Training metrics. $\lambda _R=0.00005 \text  {\nobreakspace  {}and\nobreakspace  {}} \beta _2 = 0.9$ for all runs.\relax }}{12}{table.caption.26}\protected@file@percent }
\newlabel{table-training2}{{8}{12}{Training metrics. $\lambda _R=0.00005 \text {~and~} \beta _2 = 0.9$ for all runs.\relax }{table.caption.26}{}}
\newlabel{eqn-J2}{{13}{12}{Alpha penalty}{equation.A.13}{}}
\newlabel{eqn-ASE_N2}{{13c}{12}{Alpha penalty}{equation.A.13c}{}}
\newlabel{eqn-alpha-l2}{{13e}{12}{Alpha penalty}{equation.A.13e}{}}
