\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{longtable}
\usepackage{booktabs}

\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}

 \usepackage[
 backend=biber,
 style=alphabetic,
 citestyle=authoryear
 ]{biblatex}
% \usepackage[backend=biber]{biblatex-chicago}
\addbibresource{main.bib}

\makeatletter
\DeclareRobustCommand{\KaTeX}{K%
  {%
    \setbox0\hbox{T}%
    \setbox\@tempboxa\hbox{$\m@th$%
      \csname S@\f@size\endcsname
      \fontsize\sf@size\z@
      \math@fontsfalse\selectfont
      A}%
    \@tempdima\ht0
    \advance\@tempdima-\ht\@tempboxa
    \@tempdima\strip@pt\fontdimen1\font\@tempdima
    \advance\@tempdima-.25em
    \kern\@tempdima
    \vbox to\ht0{\box\@tempboxa
      \vss}%
  }%
  \kern-.15em
  \TeX}

\makeatother

\setlength{\parindent}{0em}
\setlength{\parskip}{.5em}
\newcommand{\TeXNet}{Hunter \TeX Net}

\title{\TeXNet{} \textbar{} CSCI 350 Project Proposal}
\author{Alex Taradachuck \and Ralph ``Blake'' Vent\'{e}}
\date{March 2020}

\begin{document}

\maketitle

\begin{abstract}
  The OpenAI request for research spawned research in using Visual Neural Models
  for translating mathematical expressions into markup code. Since the debut,
  progress has been largely incremental. Singh 2018, Wang 2019, and Bender 2019
  all attribute properties of their respect models to the biases in the corpus
  and its pre-processing. We present a new dataset and a pipeline for harvesting
  examples from source files hoping to minimize these biases.
\end{abstract}

\section{Previous Work}

The prospect of accurately transcribing mathematical expression into a markup
representation is enticing because it opens the doors for bringing new life to
old mathematical texts or those for which the source code is unavailable.

A Harvard project called \emph{What you get is what you see} (\textsc{Wygiwys})
by~\cite{deng2016you} documents strategies for machine translation of
mathematical notation using an attention-based encoder-decoder neural model.
Notably, the researchers were interested in the influence of markup alone on the
efficacy of a model --- without providing explicit information about underlying
grammars \parencite[1]{deng2016you}.

Introduced in 2016 by~\cite{deng2016you}, the \texttt{im2latex100k} dataset has
led research forward by removing the overhead of heavy pre-processing. In recent
years, it has come under question as a possible bottleneck for progress.
% TODO: cite singh, bender, and wang's objections

\section{Corpus Harvesting}

\subsection{Challenges}

We harvested examples from Cornell's ArXiv archive. As the ArXiv strictly
forbids scraping, we found a patron who properly obtained copies and uploaded
them to \url{archive.org}. We wrote a script to extract the source code files,
and notably, we use the \texttt{pandoc} document parser.

\LaTeX{} and its subset, \TeX{} pose an unique set of challenges compared to
other steps of the pre-processing pipeline. \LaTeX{} is a Turing-Complete
language, equivalent in power to general purpose programming languages like C++.
It may contain loops and recursion, classifying it as a recursively enumerable
language. As such, no \LaTeX{} parser is guaranteed to terminate.
From~\cite{deng2016you}, we also reiterate that several input sequences may map
to the same output sequence, so we use the same normalization with Khan
Academy's \KaTeX{} called by their scripts.

Practically, \texttt{pandoc} handled the files from the corpus gracefully, but
due to unlinked files and mismatched, we set \texttt{timeout 5} to terminate the
program after 5 seconds of stalling. \texttt{pandoc} also handles ambiguities to
expression matching. Before, plain text not containing mathematical expressions
had to be filtered out of the data, but \texttt{pandoc} generates an abstract
syntax tree and encases the mathematical expressions in the more
``specific''\footnote{In that it generates fewer false-positives} expression
\verb|\((.*?)\)| compared to the readily-matching \texttt{\$(.*?)\$}. An example
of one such false positive that was eliminated is provided in the
appendix. % TODO: add example

Most importantly, \texttt{pandoc} expands all the macros in the source
documents. In~\cite{deng2016you} the processed formulas required an additional
step of filtering out such macros. This is one systemic bias of
\texttt{im2latex100k}.

\section{Topics}

\section{Deliverables}

\subsection{Required Objectives}

At the submission deadline, we will have the following prepared:

\begin{enumerate}
\item All data generated and normalized, building on the work of the Harvard
  team.
\item All source code containing our finished models and documentation and
\item Research paper outlining the intricacies of our models and their
  performance on generated examples.
\item Live demo of the model
\item 2 minute video
\end{enumerate}

\subsection{Stretch Goals}

Minimal interactive web front-end where a user will be able to upload an image
of a mathematical expression and receive the \LaTeX{} code associated with it. This would also serve as the platform for one of our demos.

\section{Evaluation}

We will evaluate our models with a confusion matrix, Hamming distance, and
statistic where appropriate for both - similar to the perplexity metric employed by the Harvard Paper that is a common metric in information theory and machine learning.

\printbibliography{}
\end{document}
